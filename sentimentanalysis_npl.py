# -*- coding: utf-8 -*-
"""SentimentAnalysis_NPL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yc8J8l_2DkWCqVkrzfnpWdNCCCQ1ypom
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re

import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('vader_lexicon')

# %matplotlib inline
import warnings
warnings.filterwarnings("ignore")

import os

print(os.listdir("./data"))

pd.set_option('display.max_columns',None)

US_comments = pd.read_csv('./data/UScomments.csv', error_bad_lines=False)

US_videos = pd.read_csv('./data/USvideos.csv', error_bad_lines=False)

US_videos.head()

US_videos.shape

#atributos que contém em cada video
US_videos.nunique()

US_videos.info()

US_comments.head()

US_comments.shape

#atributos de cada comentário
US_comments.nunique()

US_comments.info()

US_comments.drop(30000, inplace=True)

US_comments = US_comments.reset_index().drop('index',axis=1)

US_comments.head()

#Removendo pontuação, números e caractéres especiais
US_comments['comment_text'] = US_comments['comment_text'].str.replace("[^a-zA-Z#]", " ")

US_comments_new = US_comments[US_comments['comment_text']!= 'NaN']

#Removendo palavras curtas
US_comments_new['comment_text'] = US_comments_new['comment_text'].apply(lambda x: ' '.join([w for w in str(x).split() if len(w)>3]))

#Colocando o texto em minúsculo
US_comments_new['comment_text'] = US_comments_new['comment_text'].apply(lambda x:x.lower())

#Tokenização
tokenized_tweet = US_comments_new['comment_text'].apply(lambda x: x.split())
tokenized_tweet.head()

US_comments_new.head()

#Lematização
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

wnl = WordNetLemmatizer()

tokenized_tweet.apply(lambda x: [wnl.lemmatize(i) for i in x if i not in set(stopwords.words('english'))]) 
tokenized_tweet.head()

for i in range(len(tokenized_tweet)):
    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])

US_comments_new['comment_text'] = tokenized_tweet

from nltk.sentiment.vader import SentimentIntensityAnalyzer
sia = SentimentIntensityAnalyzer()

#Definindo os scores de sentimentos
US_comments_new['Sentiment Scores'] = US_comments_new['comment_text'].apply(lambda x:sia.polarity_scores(x)['compound'])

US_comments_new.head()

#Classificando o score de sentimento em Positivo, negativo ou neutro
US_comments_new['Sentiment'] = US_comments_new['Sentiment Scores'].apply(lambda s : 'Positive' if s > 0 else ('Neutral' if s == 0 else 'Negative'))

US_comments_new.head()

US_comments_new.Sentiment.value_counts()

#Calculando a porcentagem de comentáriso positivos em todos os vídeos
videos = []
for i in range(0,US_comments_new.video_id.nunique()):
    a = US_comments_new[(US_comments_new.video_id == US_comments_new.video_id.unique()[i]) & (US_comments_new.Sentiment == 'Positive')].count()[0]
    b = US_comments_new[US_comments_new.video_id == US_comments_new.video_id.unique()[i]]['Sentiment'].value_counts().sum()
    Percentage = (a/b)*100
    videos.append(round(Percentage,2))

#criando novo dataframe dos videos com porcentagens positivas
Positivity = pd.DataFrame(videos,US_comments_new.video_id.unique()).reset_index()

Positivity.columns = ['video_id','Positive Percentage']

Positivity.head()

all_words = ' '.join([text for text in US_comments_new['comment_text']])
from wordcloud import WordCloud
wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

all_words_posi = ' '.join([text for text in US_comments_new['comment_text'][US_comments_new.Sentiment == 'Positive']])

wordcloud_posi = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words_posi)

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud_posi, interpolation="bilinear")
plt.axis('off')
plt.show()

all_words_nega = ' '.join([text for text in US_comments_new['comment_text'][US_comments_new.Sentiment == 'Negative']])

wordcloud_nega = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words_nega)

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud_nega, interpolation="bilinear")
plt.axis('off')
plt.show()

all_words_neu = ' '.join([text for text in US_comments_new['comment_text'][US_comments_new.Sentiment == 'Neutral']])

wordcloud_neu = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words_neu)

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud_neu, interpolation="bilinear")
plt.axis('off')
plt.show()